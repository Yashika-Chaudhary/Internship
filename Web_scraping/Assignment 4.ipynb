{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48545255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bbc0a",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fb460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n",
      "\u001b[1mMost viewed videos on YouTube :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload_date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[4]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>12.85</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[7]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.16</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[14]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>6.70</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[15]</td>\n",
       "      <td>Cocomelon ‚Äì Nursery Rhymes</td>\n",
       "      <td>6.20</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[16]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.00</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[19]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>5.89</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[24]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>5.30</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Wheels on the Bus\"[25]</td>\n",
       "      <td>Cocomelon ‚Äì Nursery Rhymes</td>\n",
       "      <td>5.24</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[26]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>4.92</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors ‚Äì Colorful Eggs on a Farm\"[27]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>4.89</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[28]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>4.80</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear ‚Äì Recipe for Disaster\"[33]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.55</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[34]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>4.35</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[35]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>3.91</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[36]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.87</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[37]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.80</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[38]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.79</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[39]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.66</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[40]</td>\n",
       "      <td>Cocomelon ‚Äì Nursery Rhymes</td>\n",
       "      <td>3.64</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Thinking Out Loud\"[41]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.60</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[42]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.59</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Dark Horse\"[43]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.52</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[44]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.48</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Faded\"[45]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.45</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Perfect\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.45</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[47]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.44</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Girls Like You\"[48]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.42</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[49]</td>\n",
       "      <td>Kiddiestv Hindi ‚Äì Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.41</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Lean On\"[50]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>3.38</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Bailando\"[51]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>3.38</td>\n",
       "      <td>April 11, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[4]   \n",
       "1    2.                                   \"Despacito\"[7]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
       "3    4.                                  \"Bath Song\"[15]   \n",
       "4    5.                               \"Shape of You\"[16]   \n",
       "5    6.                              \"See You Again\"[19]   \n",
       "6    7.                \"Phonics Song with Two Words\"[24]   \n",
       "7    8.                          \"Wheels on the Bus\"[25]   \n",
       "8    9.                                \"Uptown Funk\"[26]   \n",
       "9   10.  \"Learning Colors ‚Äì Colorful Eggs on a Farm\"[27]   \n",
       "10  11.                              \"Gangnam Style\"[28]   \n",
       "11  12.   \"Masha and the Bear ‚Äì Recipe for Disaster\"[33]   \n",
       "12  13.                             \"Dame Tu Cosita\"[34]   \n",
       "13  14.                                     \"Axel F\"[35]   \n",
       "14  15.                                      \"Sugar\"[36]   \n",
       "15  16.                                       \"Roar\"[37]   \n",
       "16  17.                             \"Counting Stars\"[38]   \n",
       "17  18.                                      \"Sorry\"[39]   \n",
       "18  19.                        \"Baa Baa Black Sheep\"[40]   \n",
       "19  20.                          \"Thinking Out Loud\"[41]   \n",
       "20  21.           \"Waka Waka (This Time for Africa)\"[42]   \n",
       "21  22.                                 \"Dark Horse\"[43]   \n",
       "22  23.                             \"Lakdi Ki Kathi\"[44]   \n",
       "23  24.                                      \"Faded\"[45]   \n",
       "24  25.                                    \"Perfect\"[46]   \n",
       "25  26.                                 \"Let Her Go\"[47]   \n",
       "26  27.                             \"Girls Like You\"[48]   \n",
       "27  28.          \"Humpty the train on a fruits ride\"[49]   \n",
       "28  29.                                    \"Lean On\"[50]   \n",
       "29  30.                                   \"Bailando\"[51]   \n",
       "\n",
       "                                           Artist Upload_date  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories       12.85   \n",
       "1                                      Luis Fonsi        8.16   \n",
       "2                                     LooLoo Kids        6.70   \n",
       "3                      Cocomelon ‚Äì Nursery Rhymes        6.20   \n",
       "4                                      Ed Sheeran        6.00   \n",
       "5                                     Wiz Khalifa        5.89   \n",
       "6                                       ChuChu TV        5.30   \n",
       "7                      Cocomelon ‚Äì Nursery Rhymes        5.24   \n",
       "8                                     Mark Ronson        4.92   \n",
       "9                                     Miroshka TV        4.89   \n",
       "10                                            Psy        4.80   \n",
       "11                                     Get Movies        4.55   \n",
       "12                                      El Chombo        4.35   \n",
       "13                                     Crazy Frog        3.91   \n",
       "14                                       Maroon 5        3.87   \n",
       "15                                     Katy Perry        3.80   \n",
       "16                                    OneRepublic        3.79   \n",
       "17                                  Justin Bieber        3.66   \n",
       "18                     Cocomelon ‚Äì Nursery Rhymes        3.64   \n",
       "19                                     Ed Sheeran        3.60   \n",
       "20                                        Shakira        3.59   \n",
       "21                                     Katy Perry        3.52   \n",
       "22                                   Jingle Toons        3.48   \n",
       "23                                    Alan Walker        3.45   \n",
       "24                                     Ed Sheeran        3.45   \n",
       "25                                      Passenger        3.44   \n",
       "26                                       Maroon 5        3.42   \n",
       "27  Kiddiestv Hindi ‚Äì Nursery Rhymes & Kids Songs        3.41   \n",
       "28                                    Major Lazer        3.38   \n",
       "29                               Enrique Iglesias        3.38   \n",
       "\n",
       "                Views  \n",
       "0       June 17, 2016  \n",
       "1    January 12, 2017  \n",
       "2     October 8, 2016  \n",
       "3         May 2, 2018  \n",
       "4    January 30, 2017  \n",
       "5       April 6, 2015  \n",
       "6       March 6, 2014  \n",
       "7        May 24, 2018  \n",
       "8   November 19, 2014  \n",
       "9   February 27, 2018  \n",
       "10      July 15, 2012  \n",
       "11   January 31, 2012  \n",
       "12      April 5, 2018  \n",
       "13      June 16, 2009  \n",
       "14   January 14, 2015  \n",
       "15  September 5, 2013  \n",
       "16       May 31, 2013  \n",
       "17   October 22, 2015  \n",
       "18      June 25, 2018  \n",
       "19    October 7, 2014  \n",
       "20       June 4, 2010  \n",
       "21  February 20, 2014  \n",
       "22      June 14, 2018  \n",
       "23   December 3, 2015  \n",
       "24   November 9, 2017  \n",
       "25      July 25, 2012  \n",
       "26       May 31, 2018  \n",
       "27   January 26, 2018  \n",
       "28     March 22, 2015  \n",
       "29     April 11, 2014  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "time.sleep(5)\n",
    "\n",
    "rank =[]\n",
    "name = []\n",
    "artist = []\n",
    "upload_date = []\n",
    "views =[]\n",
    "\n",
    "try:\n",
    "    rank_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')\n",
    "    for i in rank_tags:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "    \n",
    "try:\n",
    "    name_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')\n",
    "    for i in name_tags:\n",
    "        name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "    \n",
    "try:\n",
    "    artist_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[3]')\n",
    "    for i in artist_tags:\n",
    "        artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    artist.append('-')\n",
    "    \n",
    "try:\n",
    "    date_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[4]')\n",
    "    for i in date_tags:\n",
    "        upload_date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    upload_date.append('-')\n",
    "\n",
    "try:\n",
    "    views_tags = driver.find_elements(By.XPATH, '//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]')\n",
    "    for i in views_tags:\n",
    "        views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    views.append('-')\n",
    "    \n",
    "\n",
    "print(len(rank),len(name),len(artist),len(upload_date), len(views))\n",
    "print('\\033[1m'+'Most viewed videos on YouTube :'+'\\033[0m')\n",
    "ranking = pd.DataFrame({'Rank':rank, 'Name':name, 'Artist':artist, 'Upload_date':upload_date,'Views':views})\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b9b1f",
   "metadata": {},
   "source": [
    "# 2. Scrape the details of team India‚Äôs International fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6339556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 8 8 8\n",
      "\u001b[1mIndia‚Äôs International fixtures :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur,</td>\n",
       "      <td>9 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur,</td>\n",
       "      <td>11 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st Test -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Windsor Park,</td>\n",
       "      <td>12 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd T20I -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur,</td>\n",
       "      <td>13 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur,</td>\n",
       "      <td>16 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur,</td>\n",
       "      <td>19 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd Test -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Queen's Park Oval,</td>\n",
       "      <td>20 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur,</td>\n",
       "      <td>22 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                               Series  \\\n",
       "0  1st T20I -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "1  2nd T20I -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "2  1st Test -       INDIA TOUR OF WEST INDIES 2023   \n",
       "3  3rd T20I -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "4   1st ODI -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "5   2nd ODI -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "6  2nd Test -       INDIA TOUR OF WEST INDIES 2023   \n",
       "7   3rd ODI -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "\n",
       "                                    Place         Date         Time  \n",
       "0  Shere Bangla National Stadium, Mirpur,   9 JUL 2023  1:30 PM IST  \n",
       "1  Shere Bangla National Stadium, Mirpur,  11 JUL 2023  1:30 PM IST  \n",
       "2                           Windsor Park,  12 JUL 2023  7:30 PM IST  \n",
       "3  Shere Bangla National Stadium, Mirpur,  13 JUL 2023  1:30 PM IST  \n",
       "4  Shere Bangla National Stadium, Mirpur,  16 JUL 2023  9:00 AM IST  \n",
       "5  Shere Bangla National Stadium, Mirpur,  19 JUL 2023  9:00 AM IST  \n",
       "6                      Queen's Park Oval,  20 JUL 2023  7:30 PM IST  \n",
       "7  Shere Bangla National Stadium, Mirpur,  22 JUL 2023  9:00 AM IST  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://www.bcci.tv/\")\n",
    "time.sleep(5)\n",
    "\n",
    "tab = driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]')\n",
    "tab.click()\n",
    "\n",
    "title =[]\n",
    "series =[]\n",
    "place =[]\n",
    "date = []\n",
    "times = []\n",
    "\n",
    "try:\n",
    "    title_tags = driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "    for i in title_tags:\n",
    "        title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    title.append('-')\n",
    "try:\n",
    "    series_tags = driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "    for i in series_tags:\n",
    "        series.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    series.append('-')\n",
    "try:\n",
    "    place_tags = driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]')\n",
    "    for i in place_tags:\n",
    "        place.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    place.append('-')\n",
    "try:\n",
    "    date_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "    for i in date_tags:\n",
    "        date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    date.append('-')\n",
    "try:\n",
    "    time_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "    for i in time_tags:\n",
    "        times.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    times.append('-')\n",
    "\n",
    "\n",
    "print(len(title),len(series),len(place),len(date), len(times))\n",
    "print('\\033[1m'+'India‚Äôs International fixtures :'+'\\033[0m')\n",
    "ranking = pd.DataFrame({'Match Title':title, 'Series':series, 'Place':place, 'Date':date,'Time':times})\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d363583",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP at current price (19-20)\n",
    "D) GSDP at current price (18-19)\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868da072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33\n",
      "\u001b[1mState-wise GDP of India :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>States</th>\n",
       "      <th>GSDP 19-20</th>\n",
       "      <th>GSDP 18-19</th>\n",
       "      <th>Share</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                     States GSDP 19-20 GSDP 18-19   Share      GDP\n",
       "0     1                Maharashtra          -  2,632,792  13.94%  399.921\n",
       "1     2                 Tamil Nadu  1,845,853  1,630,208   8.63%  247.629\n",
       "2     3              Uttar Pradesh  1,687,818  1,584,764   8.39%  240.726\n",
       "3     4                    Gujarat          -  1,502,899   7.96%  228.290\n",
       "4     5                  Karnataka  1,631,977  1,493,127   7.91%  226.806\n",
       "5     6                West Bengal  1,253,832  1,089,898   5.77%  165.556\n",
       "6     7                  Rajasthan  1,020,989    942,586   4.99%  143.179\n",
       "7     8             Andhra Pradesh    972,782    862,957   4.57%  131.083\n",
       "8     9                  Telangana    969,604    861,031   4.56%  130.791\n",
       "9    10             Madhya Pradesh    906,672    809,592   4.29%  122.977\n",
       "10   11                     Kerala          -    781,653   4.14%  118.733\n",
       "11   12                      Delhi    856,112    774,870   4.10%  117.703\n",
       "12   13                    Haryana    831,610    734,163   3.89%  111.519\n",
       "13   14                      Bihar    611,804    530,363   2.81%   80.562\n",
       "14   15                     Punjab    574,760    526,376   2.79%   79.957\n",
       "15   16                     Odisha    521,275    487,805   2.58%   74.098\n",
       "16   17                      Assam          -    315,881   1.67%   47.982\n",
       "17   18               Chhattisgarh    329,180    304,063   1.61%   46.187\n",
       "18   19                  Jharkhand    328,598    297,204   1.57%   45.145\n",
       "19   20                Uttarakhand          -    245,895   1.30%   37.351\n",
       "20   21            Jammu & Kashmir          -    155,956   0.83%   23.690\n",
       "21   22           Himachal Pradesh    165,472    153,845   0.81%   23.369\n",
       "22   23                        Goa     80,449     73,170   0.39%   11.115\n",
       "23   24                    Tripura     55,984     49,845   0.26%    7.571\n",
       "24   25                 Chandigarh          -     42,114   0.22%    6.397\n",
       "25   26                 Puducherry     38,253     34,433   0.18%    5.230\n",
       "26   27                  Meghalaya     36,572     33,481   0.18%    5.086\n",
       "27   28                     Sikkim     32,496     28,723   0.15%    4.363\n",
       "28   29                    Manipur     31,790     27,870   0.15%    4.233\n",
       "29   30                   Nagaland          -     27,283   0.14%    4.144\n",
       "30   31          Arunachal Pradesh          -     24,603   0.13%    3.737\n",
       "31   32                    Mizoram     26,503     22,287   0.12%    3.385\n",
       "32   33  Andaman & Nicobar Islands          -          -       -        -"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "economy = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/button')\n",
    "india = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/div/a[3]')\n",
    "economy.click()\n",
    "india.click()\n",
    "states = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "states.click()\n",
    "\n",
    "rank =[]\n",
    "state = []\n",
    "gsdp_1920 = []\n",
    "gsdp_1819 = []\n",
    "share = []\n",
    "gdp = []\n",
    "\n",
    "try:\n",
    "    rank_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "    for i in rank_tags:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "\n",
    "try:\n",
    "    state_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "    for i in state_tags:\n",
    "        state.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    state.append('-')\n",
    "\n",
    "try:\n",
    "    gsdp19_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "    for i in gsdp19_tags:\n",
    "        gsdp_1920.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gsdp_1920.append('-')\n",
    "\n",
    "try:\n",
    "    gsdp18_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "    for i in gsdp18_tags:\n",
    "        gsdp_1819.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gsdp_1819.append('-')\n",
    "try:\n",
    "    share_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "    for i in share_tags:\n",
    "        share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    share.append('-')\n",
    "try:\n",
    "    gdp_tags = driver.find_elements(By.XPATH, '//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "    for i in gdp_tags:\n",
    "        gdp.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gdp.append('-')\n",
    "\n",
    "print(len(rank),len(state),len(gsdp_1920),len(gsdp_1819), len(share),len(gdp))\n",
    "print('\\033[1m'+'State-wise GDP of India :'+'\\033[0m')\n",
    "\n",
    "gdp_states = pd.DataFrame({'Rank':rank, 'States':state, 'GSDP 19-20':gsdp_1920, 'GSDP 18-19':gsdp_1819,'Share':share,'GDP':gdp})\n",
    "gdp_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a047c",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695ec4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25\n",
      "\u001b[1mTrending repositories on Github.com :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XingangPan / DragGAN</td>\n",
       "      <td>Official Code for DragGAN (SIGGRAPH 2023)</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[Python, Cuda, C++]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THUDM / ChatGLM2-6B</td>\n",
       "      <td>ChatGLM2-6B: An Open Bilingual Chat LLM | ÂºÄÊ∫êÂèåËØ≠...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASIA-IVA-Lab / FastSAM</td>\n",
       "      <td>Fast Segment Anything</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ramonvc / freegpt-webui</td>\n",
       "      <td>GPT 3.5/4 with a Chat Web UI. No API key requi...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[Python, JavaScript, CSS, HTML, Dockerfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embedchain / embedchain</td>\n",
       "      <td>Framework to easily create LLM powered bots ov...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spacedriveapp / spacedrive</td>\n",
       "      <td>Spacedrive is an open source cross-platform fi...</td>\n",
       "      <td>[64]</td>\n",
       "      <td>[Rust, TypeScript, JavaScript, SCSS, CSS, Java]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xitanggg / open-resume</td>\n",
       "      <td>OpenResume is a powerful open-source resume bu...</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[TypeScript, CSS, JavaScript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>papers-we-love / papers-we-love</td>\n",
       "      <td>Papers from the computer science community to ...</td>\n",
       "      <td>[247]</td>\n",
       "      <td>[Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadmann7 / skateshop</td>\n",
       "      <td>An open source e-commerce skateshop build with...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[TypeScript, JavaScript, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>microsoft / Web-Dev-For-Beginners</td>\n",
       "      <td>24 Lessons, 12 Weeks, Get Started as a Web Dev...</td>\n",
       "      <td>[205]</td>\n",
       "      <td>[JavaScript, HTML, CSS, Vue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sb-ocr / diy-spacemouse</td>\n",
       "      <td>A DIY navigation device for Fusion360</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[C++, Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>THUDM / ChatGLM-6B</td>\n",
       "      <td>ChatGLM-6B: An Open Bilingual Dialogue Languag...</td>\n",
       "      <td>[44]</td>\n",
       "      <td>[Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SizheAn / PanoHead</td>\n",
       "      <td>Code Repository for CVPR 2023 Paper \"PanoHead:...</td>\n",
       "      <td>[-]</td>\n",
       "      <td>[Python, Cuda, C++, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PlexPt / awesome-chatgpt-prompts-zh</td>\n",
       "      <td>ChatGPT ‰∏≠ÊñáË∞ÉÊïôÊåáÂçó„ÄÇÂêÑÁßçÂú∫ÊôØ‰ΩøÁî®ÊåáÂçó„ÄÇÂ≠¶‰π†ÊÄé‰πàËÆ©ÂÆÉÂê¨‰Ω†ÁöÑËØù„ÄÇ</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>firstcontributions / first-contributions</td>\n",
       "      <td>üöÄ‚ú® Help beginners to contribute to open source...</td>\n",
       "      <td>[5,000+]</td>\n",
       "      <td>[-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>actualbudget / actual</td>\n",
       "      <td>A local-first personal finance system</td>\n",
       "      <td>[52]</td>\n",
       "      <td>[JavaScript, TypeScript, C++, C, M4, Hack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>xtekky / gpt4free</td>\n",
       "      <td>The official gpt4free repository | various col...</td>\n",
       "      <td>[82]</td>\n",
       "      <td>[Python, Dockerfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sveltejs / svelte</td>\n",
       "      <td>Cybernetically enhanced web apps</td>\n",
       "      <td>[610]</td>\n",
       "      <td>[JavaScript, Svelte]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Zeqiang-Lai / DragGAN</td>\n",
       "      <td>Unofficial Implementation of DragGAN - \"Drag Y...</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[Python, Cuda, C++]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OpenDriveLab / UniAD</td>\n",
       "      <td>[CVPR 2023 Best Paper] Planning-oriented Auton...</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qgis / QGIS</td>\n",
       "      <td>QGIS is a free, open source, cross platform (l...</td>\n",
       "      <td>[491]</td>\n",
       "      <td>[C++, Python, CMake, C, QML, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chat2db / Chat2DB</td>\n",
       "      <td>An intelligent and versatile general-purpose S...</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[Java, TypeScript, Less, JavaScript, HTML, EJS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Kanaries / pygwalker</td>\n",
       "      <td>PyGWalker: Turn your pandas dataframe into a T...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[Python, TypeScript, Jupyter Notebook, HTML, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ggerganov / ggml</td>\n",
       "      <td>Tensor library for machine learning</td>\n",
       "      <td>[48]</td>\n",
       "      <td>[C, Cuda, C++, Metal, Objective-C, Zig]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>StanGirard / quivr</td>\n",
       "      <td>Dump all your files and thoughts into your pri...</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[TypeScript, Python, JavaScript, PLpgSQL, Dock...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Repository Title  \\\n",
       "0                       XingangPan / DragGAN   \n",
       "1                        THUDM / ChatGLM2-6B   \n",
       "2                    CASIA-IVA-Lab / FastSAM   \n",
       "3                    ramonvc / freegpt-webui   \n",
       "4                    embedchain / embedchain   \n",
       "5                 spacedriveapp / spacedrive   \n",
       "6                     xitanggg / open-resume   \n",
       "7            papers-we-love / papers-we-love   \n",
       "8                       sadmann7 / skateshop   \n",
       "9          microsoft / Web-Dev-For-Beginners   \n",
       "10                   sb-ocr / diy-spacemouse   \n",
       "11                        THUDM / ChatGLM-6B   \n",
       "12                        SizheAn / PanoHead   \n",
       "13       PlexPt / awesome-chatgpt-prompts-zh   \n",
       "14  firstcontributions / first-contributions   \n",
       "15                     actualbudget / actual   \n",
       "16                         xtekky / gpt4free   \n",
       "17                         sveltejs / svelte   \n",
       "18                     Zeqiang-Lai / DragGAN   \n",
       "19                      OpenDriveLab / UniAD   \n",
       "20                               qgis / QGIS   \n",
       "21                         chat2db / Chat2DB   \n",
       "22                      Kanaries / pygwalker   \n",
       "23                          ggerganov / ggml   \n",
       "24                        StanGirard / quivr   \n",
       "\n",
       "                               Repository Description Contributors count  \\\n",
       "0           Official Code for DragGAN (SIGGRAPH 2023)                [9]   \n",
       "1   ChatGLM2-6B: An Open Bilingual Chat LLM | ÂºÄÊ∫êÂèåËØ≠...                [5]   \n",
       "2                               Fast Segment Anything               [10]   \n",
       "3   GPT 3.5/4 with a Chat Web UI. No API key requi...                [3]   \n",
       "4   Framework to easily create LLM powered bots ov...                [5]   \n",
       "5   Spacedrive is an open source cross-platform fi...               [64]   \n",
       "6   OpenResume is a powerful open-source resume bu...                [-]   \n",
       "7   Papers from the computer science community to ...              [247]   \n",
       "8   An open source e-commerce skateshop build with...                [5]   \n",
       "9   24 Lessons, 12 Weeks, Get Started as a Web Dev...              [205]   \n",
       "10              A DIY navigation device for Fusion360                [-]   \n",
       "11  ChatGLM-6B: An Open Bilingual Dialogue Languag...               [44]   \n",
       "12  Code Repository for CVPR 2023 Paper \"PanoHead:...                [-]   \n",
       "13                ChatGPT ‰∏≠ÊñáË∞ÉÊïôÊåáÂçó„ÄÇÂêÑÁßçÂú∫ÊôØ‰ΩøÁî®ÊåáÂçó„ÄÇÂ≠¶‰π†ÊÄé‰πàËÆ©ÂÆÉÂê¨‰Ω†ÁöÑËØù„ÄÇ               [19]   \n",
       "14  üöÄ‚ú® Help beginners to contribute to open source...           [5,000+]   \n",
       "15              A local-first personal finance system               [52]   \n",
       "16  The official gpt4free repository | various col...               [82]   \n",
       "17                   Cybernetically enhanced web apps              [610]   \n",
       "18  Unofficial Implementation of DragGAN - \"Drag Y...                [9]   \n",
       "19  [CVPR 2023 Best Paper] Planning-oriented Auton...                [6]   \n",
       "20  QGIS is a free, open source, cross platform (l...              [491]   \n",
       "21  An intelligent and versatile general-purpose S...                [7]   \n",
       "22  PyGWalker: Turn your pandas dataframe into a T...               [11]   \n",
       "23                Tensor library for machine learning               [48]   \n",
       "24  Dump all your files and thoughts into your pri...               [26]   \n",
       "\n",
       "                                        Language used  \n",
       "0                                 [Python, Cuda, C++]  \n",
       "1                                            [Python]  \n",
       "2                                            [Python]  \n",
       "3         [Python, JavaScript, CSS, HTML, Dockerfile]  \n",
       "4                                            [Python]  \n",
       "5     [Rust, TypeScript, JavaScript, SCSS, CSS, Java]  \n",
       "6                       [TypeScript, CSS, JavaScript]  \n",
       "7                                             [Shell]  \n",
       "8                       [TypeScript, JavaScript, CSS]  \n",
       "9                        [JavaScript, HTML, CSS, Vue]  \n",
       "10                                      [C++, Python]  \n",
       "11                                    [Python, Shell]  \n",
       "12                         [Python, Cuda, C++, Shell]  \n",
       "13                                                [-]  \n",
       "14                                                [-]  \n",
       "15         [JavaScript, TypeScript, C++, C, M4, Hack]  \n",
       "16                               [Python, Dockerfile]  \n",
       "17                               [JavaScript, Svelte]  \n",
       "18                                [Python, Cuda, C++]  \n",
       "19                                    [Python, Shell]  \n",
       "20                [C++, Python, CMake, C, QML, Shell]  \n",
       "21    [Java, TypeScript, Less, JavaScript, HTML, EJS]  \n",
       "22  [Python, TypeScript, Jupyter Notebook, HTML, S...  \n",
       "23            [C, Cuda, C++, Metal, Objective-C, Zig]  \n",
       "24  [TypeScript, Python, JavaScript, PLpgSQL, Dock...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://github.com/\")\n",
    "time.sleep(5)\n",
    "\n",
    "src = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button\")\n",
    "src.click()\n",
    "trending = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a\")\n",
    "trending.click()\n",
    "time.sleep(10)\n",
    "\n",
    "title = []\n",
    "desc = []\n",
    "count = []\n",
    "lang = []\n",
    "\n",
    "URL= []\n",
    "link=driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]/a')\n",
    "for i in link:\n",
    "    URL.append(i.get_attribute('href'))\n",
    "\n",
    "try:\n",
    "    title_tags = driver.find_elements(By.XPATH,'/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article/h2')\n",
    "    for i in title_tags:\n",
    "        title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    title.append('-')\n",
    "\n",
    "for i in URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        desc_tags = driver.find_elements(By.XPATH,'//*[@class=\"f4 my-3\" or @class= \"f4 my-3 color-fg-muted text-italic\"]')\n",
    "        for i in desc_tags:\n",
    "            desc.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        desc.append('-')\n",
    "    countlist = []\n",
    "    try:\n",
    "        count_tags = driver.find_elements(By.XPATH,'//h2[@class=\"h4 mb-3\"]/a[contains(text(),\"Contributors\")]/span')\n",
    "        if count_tags:\n",
    "            for j in count_tags:\n",
    "                countlist.append(j.text)\n",
    "        else:\n",
    "            countlist.append('-')\n",
    "        count.append(countlist)\n",
    "    except NoSuchElementException:\n",
    "        count.append('-')\n",
    "    langlist =[]\n",
    "    try:\n",
    "        lang_tags = driver.find_elements(By.XPATH,'//li[@class=\"d-inline\"]//a//span[1]')\n",
    "        if lang_tags:\n",
    "            for j in lang_tags:\n",
    "                langlist.append(j.text)\n",
    "        else:\n",
    "            langlist.append('-')\n",
    "        lang.append(langlist)\n",
    "    except NoSuchElementException:\n",
    "        lang.append('-')\n",
    "\n",
    "print(len(title),len(desc),len(count),len(lang))\n",
    "print('\\033[1m'+'Trending repositories on Github.com :'+'\\033[0m')\n",
    "\n",
    "git = pd.DataFrame({'Repository Title':title, 'Repository Description':desc,'Contributors count':count,'Language used':lang})\n",
    "git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a95c8",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95191c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n",
      "\u001b[1mTop 100 songs on billiboard.com :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Angel, Pt. 1</td>\n",
       "      <td>Kodak Black, NLE Choppa, Jimin, JVKE &amp; Muni Long</td>\n",
       "      <td>-</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Girl In Mine</td>\n",
       "      <td>Parmalee</td>\n",
       "      <td>-</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Moonlight</td>\n",
       "      <td>Kali Uchis</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Classy 101</td>\n",
       "      <td>Feid x Young Miko</td>\n",
       "      <td>-</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bluffin</td>\n",
       "      <td>Gucci Mane &amp; Lil Baby</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Song Name                                       Artist Name  \\\n",
       "0     Last Night                                     Morgan Wallen   \n",
       "1       Fast Car                                        Luke Combs   \n",
       "2      Calm Down                               Rema & Selena Gomez   \n",
       "3        Flowers                                       Miley Cyrus   \n",
       "4    All My Life                        Lil Durk Featuring J. Cole   \n",
       "..           ...                                               ...   \n",
       "95  Angel, Pt. 1  Kodak Black, NLE Choppa, Jimin, JVKE & Muni Long   \n",
       "96  Girl In Mine                                          Parmalee   \n",
       "97     Moonlight                                        Kali Uchis   \n",
       "98    Classy 101                                 Feid x Young Miko   \n",
       "99       Bluffin                             Gucci Mane & Lil Baby   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks on Chart  \n",
       "0               1         1             21  \n",
       "1               3         2             13  \n",
       "2               4         3             42  \n",
       "3               2         1             23  \n",
       "4               5         2              6  \n",
       "..            ...       ...            ...  \n",
       "95              -        65              2  \n",
       "96              -        97              1  \n",
       "97             90        80             11  \n",
       "98              -        99              1  \n",
       "99              -       100              1  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"http://www.billboard.com/\")\n",
    "\n",
    "charts = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "charts.click()\n",
    "\n",
    "hot100 = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/main/div[2]/div[1]/div[1]/div/div/div[3]/a')\n",
    "hot100.click()\n",
    "time.sleep(5)\n",
    "\n",
    "song_name = []\n",
    "artist_name = []\n",
    "lastweek_rank =[]\n",
    "peak_rank = []\n",
    "weekson_board = []\n",
    "\n",
    "try:\n",
    "    sname = driver.find_elements(By.XPATH, '//h3[@class = \"c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 u-font-size-23@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-245 u-max-width-230@tablet-only u-letter-spacing-0028@tablet\" or @class = \"c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 lrv-u-font-size-18@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-330 u-max-width-230@tablet-only\"]')\n",
    "    for i in sname:\n",
    "        song_name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    song_name.append('-')\n",
    "\n",
    "try:\n",
    "    aname = driver.find_elements(By.XPATH,'//span[@class = \"c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet\" or @class = \"c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only\"]')\n",
    "    for i in aname:\n",
    "        artist_name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    artist_name.append('-')\n",
    "    \n",
    "\n",
    "try:\n",
    "    lrank = driver.find_elements(By.XPATH, '//div[@class = \"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[4]')\n",
    "    for i in lrank:\n",
    "        lastweek_rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    lastweek_rank.append('-')\n",
    "\n",
    "\n",
    "try:\n",
    "    prank = driver.find_elements(By.XPATH,'//div[@class = \"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[5]')\n",
    "    for i in prank:\n",
    "        peak_rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    peak_rank.append('-')\n",
    "\n",
    "try:\n",
    "    week = driver.find_elements(By.XPATH,'//div[@class = \"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[6]')\n",
    "    for i in week:\n",
    "        weekson_board.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    weekson_board.append('-')\n",
    "    \n",
    "print(len(song_name),len(artist_name),len(lastweek_rank),len(peak_rank),len(weekson_board))\n",
    "print('\\033[1m'+'Top 100 songs on billiboard.com :'+'\\033[0m')\n",
    "top100 = pd.DataFrame({'Song Name':song_name,'Artist Name': artist_name,'Last Week Rank':lastweek_rank,'Peak Rank':peak_rank,'Weeks on Chart':weekson_board})\n",
    "top100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c3989",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-\n",
    "compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a496338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n",
      "\u001b[1mHighest selling novels:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name       Artist Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes Sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "\n",
    "bname = []\n",
    "aname = []\n",
    "volume = []\n",
    "publisher = []\n",
    "genre = []\n",
    "\n",
    "try:\n",
    "    book = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[2]')\n",
    "    for i in book:\n",
    "        bname.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    bname.append('-')\n",
    "try:\n",
    "    auth = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[3]')\n",
    "    for i in auth:\n",
    "        aname.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    aname.append('-')\n",
    "try:\n",
    "    vol = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[4]')\n",
    "    for i in vol:\n",
    "        volume.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    volume.append('-')\n",
    "try:\n",
    "    pub = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[5]')\n",
    "    for i in pub:\n",
    "        publisher.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    publisher.append('-')\n",
    "try:\n",
    "    gen = driver.find_elements(By.XPATH,'//*[@id=\"article-body-blocks\"]/div/table/tbody/tr/td[6]')\n",
    "    for i in gen:\n",
    "        genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    genre.append('-')\n",
    "\n",
    "print(len(bname),len(aname),len(volume),len(publisher),len(genre))\n",
    "print('\\033[1m'+'Highest selling novels:'+'\\033[0m')\n",
    "bestselling = pd.DataFrame({'Book Name':bname,'Artist Name': aname,'Volumes Sold':volume,'Publisher':publisher,'Genre':genre})\n",
    "bestselling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1845afa",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f0d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100 100\n",
      "\u001b[1mMost watched tv series of all time :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011‚Äì2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,173,452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016‚Äì2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,251,325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010‚Äì2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,032,387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017‚Äì2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>303,530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014‚Äì2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>262,710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013‚Äì2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>51,952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017‚Äì2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>63,993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005‚Äì )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>208,526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015‚Äì2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>43,395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>260,143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Show Name         Year                     Genre  \\\n",
       "0                  Game of Thrones  (2011‚Äì2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things  (2016‚Äì2024)    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010‚Äì2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017‚Äì2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014‚Äì2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013‚Äì2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017‚Äì2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds     (2005‚Äì )     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015‚Äì2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "    Runtime Rating      Votes  \n",
       "0    57 min    9.2  2,173,452  \n",
       "1    51 min    8.7  1,251,325  \n",
       "2    44 min    8.1  1,032,387  \n",
       "3    60 min    7.5    303,530  \n",
       "4    43 min    7.6    262,710  \n",
       "..      ...    ...        ...  \n",
       "95   42 min    7.4     51,952  \n",
       "96   50 min    7.8     63,993  \n",
       "97   42 min    8.1    208,526  \n",
       "98   45 min    7.1     43,395  \n",
       "99  572 min    8.6    260,143  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "name = []\n",
    "year = []\n",
    "genres = []\n",
    "runtime = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "try:\n",
    "    name_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/h3/a')\n",
    "    for i in name_tags:\n",
    "        name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "try:\n",
    "    year_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/h3/span[2]')\n",
    "    for i in year_tags:\n",
    "        year.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    year.append('-')\n",
    "try:\n",
    "    genre_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/p[1]/span[5]')\n",
    "    for i in genre_tags:\n",
    "        genres.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    genres.append('-')\n",
    "try:\n",
    "    run_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/p[1]/span[3]')\n",
    "    for i in run_tags:\n",
    "        runtime.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    runtime.append('-')\n",
    "try:\n",
    "    rating_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/div[1]/div[1]/span[2]')\n",
    "    for i in rating_tags:\n",
    "        ratings.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    ratings.append('-')\n",
    "try:\n",
    "    vote_tags = driver.find_elements(By.XPATH,'//div[@class = \"lister-item-content\"]/p[4]/span[2]')\n",
    "    for i in vote_tags:\n",
    "        votes.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    votes.append('-')\n",
    "\n",
    "print(len(name),len(year),len(genres),len(runtime),len(ratings),len(votes))\n",
    "print('\\033[1m'+'Most watched tv series of all time :'+'\\033[0m')\n",
    "\n",
    "imdb = pd.DataFrame({'Show Name':name,'Year': year,'Genre':genres,'Runtime':runtime, 'Rating':ratings,'Votes':votes})\n",
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d6e84",
   "metadata": {},
   "source": [
    "# 8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20ba2303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623 623 623 623 623 623 623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Number of instances</th>\n",
       "      <th>Number of attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Attributes</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Attributes</td>\n",
       "      <td>7/1/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>17 Attributes</td>\n",
       "      <td>9/14/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>Classification</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Attributes</td>\n",
       "      <td>5/1/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td></td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>20 Attributes</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Undocumented</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>PMU-UD</td>\n",
       "      <td>Univariate</td>\n",
       "      <td></td>\n",
       "      <td>Classification</td>\n",
       "      <td>5.18K Instances</td>\n",
       "      <td>9 Attributes</td>\n",
       "      <td>8/5/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>EBL Domain Theories</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Moral Reasoner</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td>202 Instances</td>\n",
       "      <td></td>\n",
       "      <td>6/1/1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>DGP2 - The Second Data Generation Program</td>\n",
       "      <td></td>\n",
       "      <td>Real</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Dataset Name     Data Type  \\\n",
       "0                                         Iris  Multivariate   \n",
       "1                                Heart Disease  Multivariate   \n",
       "2                             Dry Bean Dataset  Multivariate   \n",
       "3                                        Adult  Multivariate   \n",
       "4                                     Diabetes                 \n",
       "..                                         ...           ...   \n",
       "618                               Undocumented                 \n",
       "619                                     PMU-UD    Univariate   \n",
       "620                        EBL Domain Theories                 \n",
       "621                             Moral Reasoner                 \n",
       "622  DGP2 - The Second Data Generation Program                 \n",
       "\n",
       "                 Attribute Type            Task Number of instances  \\\n",
       "0                          Real  Classification       150 Instances   \n",
       "1    Categorical, Integer, Real  Classification       303 Instances   \n",
       "2                 Integer, Real  Classification    13.61K Instances   \n",
       "3          Categorical, Integer  Classification    48.84K Instances   \n",
       "4          Categorical, Integer                                       \n",
       "..                          ...             ...                 ...   \n",
       "618                         N/A                                       \n",
       "619                              Classification     5.18K Instances   \n",
       "620                         N/A                                       \n",
       "621                         N/A                       202 Instances   \n",
       "622                        Real                                       \n",
       "\n",
       "    Number of attributes       Year  \n",
       "0           4 Attributes   7/1/1988  \n",
       "1          13 Attributes   7/1/1988  \n",
       "2          17 Attributes  9/14/2020  \n",
       "3          14 Attributes   5/1/1996  \n",
       "4          20 Attributes        N/A  \n",
       "..                   ...        ...  \n",
       "618                             N/A  \n",
       "619         9 Attributes   8/5/2018  \n",
       "620                             N/A  \n",
       "621                        6/1/1994  \n",
       "622                             N/A  \n",
       "\n",
       "[623 rows x 7 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "time.sleep(5)\n",
    "\n",
    "view = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "view.click()\n",
    "\n",
    "time.sleep(20)\n",
    "accept = driver.find_element(By.XPATH,'//*[@class = \"btn-primary btn-sm btn m-1\"]')\n",
    "accept.click()\n",
    "time.sleep (30)\n",
    "\n",
    "expand_all = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/label[2]/div[2]/span[2]')\n",
    "expand_all.click()\n",
    "time.sleep(20)\n",
    "\n",
    "name =[]\n",
    "data_type =[]\n",
    "task_type = []\n",
    "attribute_type = []\n",
    "no_instances = []\n",
    "no_attribute = []\n",
    "year = []\n",
    "\n",
    "for page in range(0,63):\n",
    "    driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "    try:\n",
    "        name_tags = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/h2/a')\n",
    "        for i in name_tags:\n",
    "            name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append('-')\n",
    "\n",
    "    try:\n",
    "        data = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[2]/span')\n",
    "        for i in data:\n",
    "            data_type.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        data_type.append('-')\n",
    "\n",
    "    try:\n",
    "        task = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[1]/span')\n",
    "        for i in task:\n",
    "            task_type.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        task_type.append('-')\n",
    "    try:\n",
    "        instances = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[3]/span')\n",
    "        for i in instances:\n",
    "            no_instances.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        no_instances.append('-')\n",
    "    try:\n",
    "        attribute_tags = driver.find_elements(By.XPATH,'//div[@class = \"relative col-span-8 sm:col-span-7\"]/div/div[4]/span')\n",
    "        for i in attribute_tags:\n",
    "            no_attribute.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        no_attribute.append('-')\n",
    "\n",
    "    try:\n",
    "        attribute = driver.find_elements(By.XPATH,'//div[@class = \"rounded-box bg-base-100\"]/div[2]/div/table/tbody/tr/td[2]')\n",
    "        for i in attribute:\n",
    "            attribute_type.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        attribute_type.append('-')\n",
    "    try:\n",
    "        year_tags = driver.find_elements(By.XPATH,'//div[@class = \"rounded-box bg-base-100\"]/div[2]/div/table/tbody/tr/td[3]')\n",
    "        for i in year_tags:\n",
    "            year.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        year.append('-')\n",
    "    if(page == 62):\n",
    "        break\n",
    "    else:\n",
    "        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//div[@class =\"btn-group\"]/button[2]'))).click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "print(len(name),len(data_type),len(task_type),len(attribute_type),len(no_instances),len(no_attribute),len(year))\n",
    "\n",
    "uci = pd.DataFrame({'Dataset Name':name,'Data Type':data_type,'Attribute Type':attribute_type, 'Task': task_type,'Number of instances':no_instances,'Number of attributes':no_attribute,'Year': year})\n",
    "uci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b43939",
   "metadata": {},
   "source": [
    "# 9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company\n",
    "D)Skills they hire for\n",
    "E) Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986e9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20\n",
      "\u001b[1mData science recruiters  :\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>location</th>\n",
       "      <th>Skills they hire</th>\n",
       "      <th>Recruiter Name</th>\n",
       "      <th>Designation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bizongo</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Vision,Analytics,Deep Learning,Networking,Tens...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Raytheon Technologies</td>\n",
       "      <td>Hybrid - Bangalore/ Bengaluru, Karnataka(Yelah...</td>\n",
       "      <td>Data Science,statistical modeling,machine lear...</td>\n",
       "      <td>Shilpa Sk</td>\n",
       "      <td>HR Executive at Pratt &amp; Whitney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Augusta Infotech</td>\n",
       "      <td>Bangalore/ Bengaluru, Karnataka(Electronic City)</td>\n",
       "      <td>python,numpy,machine learning,tensorflow,Panda...</td>\n",
       "      <td>Biju John</td>\n",
       "      <td>Director - Recruitment at Augusta Infotech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUN BRADSTREET INFORMATION SERVICES INDIA PRIV...</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "      <td>Analysis,Analytical,Data Science,Data analysis...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVE-Promagne Business Solutions</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>Data Analytics,IT training,Training,Machine,De...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Machine learning,Predictive modeling,Data mini...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Data Science,Predictive Modeling,machine learn...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zupee</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>Machine Learning,Neural networks,Deep Learning...</td>\n",
       "      <td>Aksshay Malaviya</td>\n",
       "      <td>Manager Talent Acquisition at CASHGRAIL PRIVAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Epiq Systems</td>\n",
       "      <td>Hybrid - Hyderabad/Secunderabad</td>\n",
       "      <td>Data Science,Spark,Scikit-learn,Keras,Software...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Epiq Systems, Inc.</td>\n",
       "      <td>Hyderabad/Secunderabad, Canada, Pune</td>\n",
       "      <td>Languages,Development,Product management,Data ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Venator Holdings</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "      <td>Recruitment,Senior,Statistical modeling,Data S...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>QA InfoTech Pvt. Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cloud,Cloud computing,Data Science,Simulation,...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>QA InfoTech Pvt. Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Ml,MySQL,SDLC,JIRA,Data Science,Cloud computin...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Conneqt Digital</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru</td>\n",
       "      <td>Data Science,Azure,Artificial Intelligence,Azu...</td>\n",
       "      <td>Pavithra TR</td>\n",
       "      <td>Senior Technical Recruiter at Conneqt Business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Data Science,Python,Intelligence,Data analysis...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Data Science,Artificial Intelligence,Data mini...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Merilytics</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "      <td>Decision Tree,Natural Language Processing,Neur...</td>\n",
       "      <td>Suryakalla Mattaparthy</td>\n",
       "      <td>Senior Manager at Merilytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Epiq Global</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune</td>\n",
       "      <td>Django,Natural language processing,UI developm...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TRH Consultancy Services</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "      <td>Data Science,Java,R,Machine,Algorithms,Data,Da...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Machine Learning,Machine,Python,Science,Operat...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Company  \\\n",
       "0                                             Bizongo   \n",
       "1                               Raytheon Technologies   \n",
       "2                                    Augusta Infotech   \n",
       "3   DUN BRADSTREET INFORMATION SERVICES INDIA PRIV...   \n",
       "4                     AVE-Promagne Business Solutions   \n",
       "5                                           Accenture   \n",
       "6                                           Accenture   \n",
       "7                                               Zupee   \n",
       "8                                        Epiq Systems   \n",
       "9                                  Epiq Systems, Inc.   \n",
       "10                                   Venator Holdings   \n",
       "11                               QA InfoTech Pvt. Ltd   \n",
       "12                               QA InfoTech Pvt. Ltd   \n",
       "13                                    Conneqt Digital   \n",
       "14                                          Accenture   \n",
       "15                                          Accenture   \n",
       "16                                         Merilytics   \n",
       "17                                        Epiq Global   \n",
       "18                           TRH Consultancy Services   \n",
       "19                                            Red Hat   \n",
       "\n",
       "                                             location  \\\n",
       "0                                 Bangalore/Bengaluru   \n",
       "1   Hybrid - Bangalore/ Bengaluru, Karnataka(Yelah...   \n",
       "2    Bangalore/ Bengaluru, Karnataka(Electronic City)   \n",
       "3                              Hyderabad/Secunderabad   \n",
       "4   Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "5                                 Bangalore/Bengaluru   \n",
       "6                                              Mumbai   \n",
       "7                                         Delhi / NCR   \n",
       "8                     Hybrid - Hyderabad/Secunderabad   \n",
       "9                Hyderabad/Secunderabad, Canada, Pune   \n",
       "10                             Hyderabad/Secunderabad   \n",
       "11                                Bangalore/Bengaluru   \n",
       "12                                Bangalore/Bengaluru   \n",
       "13                       Hybrid - Bangalore/Bengaluru   \n",
       "14                                             Mumbai   \n",
       "15                                             Mumbai   \n",
       "16                             Hyderabad/Secunderabad   \n",
       "17                       Hyderabad/Secunderabad, Pune   \n",
       "18  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...   \n",
       "19                                Bangalore/Bengaluru   \n",
       "\n",
       "                                     Skills they hire          Recruiter Name  \\\n",
       "0   Vision,Analytics,Deep Learning,Networking,Tens...                       -   \n",
       "1   Data Science,statistical modeling,machine lear...               Shilpa Sk   \n",
       "2   python,numpy,machine learning,tensorflow,Panda...               Biju John   \n",
       "3   Analysis,Analytical,Data Science,Data analysis...                       -   \n",
       "4   Data Analytics,IT training,Training,Machine,De...                       -   \n",
       "5   Machine learning,Predictive modeling,Data mini...                       -   \n",
       "6   Data Science,Predictive Modeling,machine learn...                       -   \n",
       "7   Machine Learning,Neural networks,Deep Learning...        Aksshay Malaviya   \n",
       "8   Data Science,Spark,Scikit-learn,Keras,Software...                       -   \n",
       "9   Languages,Development,Product management,Data ...                       -   \n",
       "10  Recruitment,Senior,Statistical modeling,Data S...                       -   \n",
       "11  Cloud,Cloud computing,Data Science,Simulation,...                       -   \n",
       "12  Ml,MySQL,SDLC,JIRA,Data Science,Cloud computin...                       -   \n",
       "13  Data Science,Azure,Artificial Intelligence,Azu...             Pavithra TR   \n",
       "14  Data Science,Python,Intelligence,Data analysis...                       -   \n",
       "15  Data Science,Artificial Intelligence,Data mini...                       -   \n",
       "16  Decision Tree,Natural Language Processing,Neur...  Suryakalla Mattaparthy   \n",
       "17  Django,Natural language processing,UI developm...                       -   \n",
       "18  Data Science,Java,R,Machine,Algorithms,Data,Da...                       -   \n",
       "19  Machine Learning,Machine,Python,Science,Operat...                       -   \n",
       "\n",
       "                                          Designation  \n",
       "0                                                   -  \n",
       "1                     HR Executive at Pratt & Whitney  \n",
       "2          Director - Recruitment at Augusta Infotech  \n",
       "3                                                   -  \n",
       "4                                                   -  \n",
       "5                                                   -  \n",
       "6                                                   -  \n",
       "7   Manager Talent Acquisition at CASHGRAIL PRIVAT...  \n",
       "8                                                   -  \n",
       "9                                                   -  \n",
       "10                                                  -  \n",
       "11                                                  -  \n",
       "12                                                  -  \n",
       "13  Senior Technical Recruiter at Conneqt Business...  \n",
       "14                                                  -  \n",
       "15                                                  -  \n",
       "16                       Senior Manager at Merilytics  \n",
       "17                                                  -  \n",
       "18                                                  -  \n",
       "19                                                  -  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(\"https://www.naukri.com/hr-recruiters-consultants\")\n",
    "time.sleep(5)\n",
    "click = driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[3]/div[2]/div[1]/div')\n",
    "click.click()\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[3]/div[2]/div[1]/div/div/div[1]/div/div/div/input')\n",
    "search.clear()\n",
    "search.send_keys('Data Science')\n",
    "\n",
    "button = driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[3]/div[2]/div[1]/div/button')\n",
    "button.click()\n",
    "time.sleep(5)\n",
    "\n",
    "name = []\n",
    "designation = []\n",
    "company = []\n",
    "skills =[]\n",
    "location = []\n",
    "url = []\n",
    "\n",
    "\n",
    "try:\n",
    "    url_tags = driver.find_elements(By.XPATH,'//a[@class = \"title ellipsis\"]')\n",
    "    for i in url_tags:\n",
    "        url.append(i.get_attribute('href'))\n",
    "except NoSuchElementException:\n",
    "    url.append('-')\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "    for i in location_tags:\n",
    "        location.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    location.append('-')\n",
    "\n",
    "\n",
    "try:\n",
    "    company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in company_tags:\n",
    "        company.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    company.append('-')\n",
    "\n",
    "skill = []\n",
    "try:\n",
    "    skill_tags = driver.find_elements(By.XPATH,'//*[@class=\"tags has-description\"]')\n",
    "    for i in skill_tags:\n",
    "        skill.append(i.text.replace('\\n',','))\n",
    "except NoSuchElementException:\n",
    "    skill.append('-')\n",
    "\n",
    "name = []\n",
    "designation = []\n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        name_tags = driver.find_element(By.XPATH,'//div[@class = \"name-designation\"]')\n",
    "        name.append(name_tags.text.split('\\n')[0])\n",
    "        designation.append(name_tags.text.split('\\n')[1])\n",
    "    except NoSuchElementException:\n",
    "        name.append('-')\n",
    "        designation.append('-')\n",
    "        \n",
    "print(len(name),len(designation),len(company),len(location),len(skill))\n",
    "print('\\033[1m'+'Data science recruiters  :'+'\\033[0m')\n",
    "ds = pd.DataFrame({'Company':company,'location':location,'Skills they hire':skill,'Recruiter Name':name,'Designation':designation})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1419e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
